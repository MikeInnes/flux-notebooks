{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition midpoints(Base.Range{T} where T) in module Base at deprecated.jl:56 overwritten in module StatsBase at /home/mikeinnes/.julia/v0.6/StatsBase/src/hist.jl:535.\n",
      "WARNING: Method definition midpoints(AbstractArray{T, 1} where T) in module Base at deprecated.jl:56 overwritten in module StatsBase at /home/mikeinnes/.julia/v0.6/StatsBase/src/hist.jl:533.\n"
     ]
    }
   ],
   "source": [
    "using Flux, CuArrays\n",
    "using Flux: onehot, argmax, chunk, batchseq, throttle, crossentropy\n",
    "using StatsBase: wsample\n",
    "using Base.Iterators: partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load text data from `input.txt` and split it into characters, then turn it into the numeric form needed by the model.\n",
    "\n",
    "The model will take a sequence of characters, like \"the do\", and try to produce the next character (e.g. 't' or 'g' would be likely here but not 'd'). The target output sequence $Y$ is therefore just the input sequence $X$ offset by one, e.g.\n",
    "\n",
    "* $X$: `the dog`\n",
    "* $Y$: `he dog_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = collect(readstring(\"julia.jl\"))\n",
    "alphabet = [unique(text)..., '_']\n",
    "text = map(ch -> onehot(ch, alphabet), text)\n",
    "stop = onehot('_', alphabet)\n",
    "\n",
    "N = length(alphabet)\n",
    "seqlen = 50\n",
    "nbatch = 50\n",
    "\n",
    "Xs = collect(partition(batchseq(chunk(text, nbatch), stop), seqlen))\n",
    "Ys = collect(partition(batchseq(chunk(text[2:end], nbatch), stop), seqlen));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model will be a multi-layer LSTM, which takes a single character as input and produces a single character as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Chain(\n",
    "  LSTM(N, 128),\n",
    "  LSTM(128, 128),\n",
    "  Dense(128, N),\n",
    "  softmax)\n",
    "\n",
    "m = cu(m)\n",
    "\n",
    "function loss(xs, ys)\n",
    "  l = sum(crossentropy.(m.(cu.(collect.(xs))), cu.(ys)))\n",
    "  Flux.truncate!(m)\n",
    "  return l\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model accepts a one-hot-encoded character and returns a probability distribution over possible subsequent characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(x) = m(cu(collect(x)))\n",
    "probabilities = predict(onehot('a', alphabet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sample from this distribution to see what the model thinks comes after 'a'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsample(alphabet, probabilities.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we feed the model's output back into itself, we can allow it to \"dream\" a sequence of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sample(m, alphabet, len; temp = 1)\n",
    "  Flux.reset!(m)\n",
    "  buf = IOBuffer()\n",
    "  c = rand('a':'z')\n",
    "  for i = 1:len\n",
    "    write(buf, c)\n",
    "    c = wsample(alphabet, m(cu(collect(onehot(c, alphabet)))).data)\n",
    "  end\n",
    "  return String(take!(buf))\n",
    "end\n",
    "\n",
    "sample(m, alphabet, 100) |> println"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now it's more-or-less random because the model hasn't seen any data. Let's fix that.\n",
    "\n",
    "We just need to call `Flux.train!` with an optimiser and the data we prepared. We set up a call back so that every 30 seconds, we get to see a sample of the model's output, which you should see learning a basic words and grammar fairly quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "opt = ADAM(params(m))\n",
    "evalcb = function ()\n",
    "  print_with_color(:blue, \"Loss is $(loss(Xs[5], Ys[5]))\\n\")\n",
    "  println(sample(deepcopy(m), alphabet, 500))\n",
    "end\n",
    "@time for i = 1:5 Flux.train!(loss, zip(Xs, Ys), opt, cb = throttle(evalcb, 10)) end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc(); CuArrays.clearpool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open(io -> serialize(io, (alphabet, m)), \"julia-90.jls\", \"w\")\n",
    "# (alphabet, m) = open(deserialize, \"julia-0.90.jls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.3-pre",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
